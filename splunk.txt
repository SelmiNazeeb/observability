	----------
  	  SPLUNK
	----------
Splunk is a complete data analytics tool that doesn’t just create dashboards, but also collects data from different sources, stores it in an organized way, and lets you search, analyze, and understand that data. It can handle logs, metrics, and traces from almost any system.

- Splunk instance
- App home
	> This is the main starting page for a specific Splunk App
	> App in Splunk is like a Set of features built to solve a specific problem
	> Apps are defined by a user with an administrator role
	> EG: there is an app for security monitoring and another for IT operations. When you open an app, you land on its "App Home" page.

- Roles : Roles are a way to manage what users can and cannot do in Splunk
  > Administrator Role
	- Can do everything
	- create knowledge objects for users ()
	- Full control over the Splunk instance, including managing all users, roles, system settings, and installing apps
  > Power Role
	- can create and share all knowledge objects and perform real time searches ()
	- Can create and share reports, dashboards, and alerts
	- Can "tag" events to make searching easier for others
  > User Role
	- see only own knowledge objects and those shared with them ()
	- Can search and run their own reports and dashboards. But can't share to others
	- view data they are given access to

knowledge objects
-------------------
- Saved Searches, Reports & Dashboards, Tags, look up tables
- lookup table : your Splunk data don't have a a column u want . u created another file with 2 columns in which one is in Splunk data and other is new. then 
                 said Splunk to compare both and ask for some search u want and got the information u want
- tags :  you can use a single tag to find all related events instead of multiple terms. eg; u want 'failed' and 'denied' logs, u can create a tags named 	  'authentication failure' for failed and denied. then whenever u want u can just use that tag.
- reports and dashboard : u search some data using a complex command and save that as a report/dashboard. then if u want that again u can just click on that 			  report or if any friend want to see, u can share the report to them.

- Table views
- transforming commands : commands that create statistics and visualization 
- By default a search job will remain active for 10 minutes
- shared search job will remain active for 7 days
- export formats : xml, raw data, Jason, csv
- search modes : Fast mode, smart mode, Verbose mode
	> Fast
	  - field discovery disabled
   	  - designed for performance
	  - returns your search results as quickly as possible
	  - It only returns the essential, default fields and fields you explicitly search for
	> verbose
	  - designed for completeness
	  - It will return as much information as possible from your data, but it will take longer to run.
	  - best for Troubleshooting, debugging
	> smart
	  - automatically decides whether to act like Fast or Verbose mode depends on your search
	  - If your search includes a command that creates a report (like stats or chart), it behaves like Fast Mode to give you the results quickly.
	  - If you're just looking for events without a reporting command, it acts like Verbose Mode to show you all the available details.

searching terms
---------------
- wild card
	Examples:-
	- error*: This will find any words that start with "error," such as error, errors, error_code, and erroneous.
	- *fail*: This will find any words that contain "fail," such as failed, failure, login_fail, and fatal_fail.
	- host-web-??: While not a wildcard, you can use ? in some systems to represent a single character, as in this example where you could search for 	  host-web-01 or host-web-02
- Boolean operation - NOT, OR, AND . parenthesis () can use to control the evaluation
- escape character in search (\)

search language of Splunk - build from 5 components
----------------------------------------------------
search terms : foundation of search queries. starting point of every search. (login)
commands : tells what we want to do with search results (creating charts, computing statistics, formatting)
	   You use a pipe (|) to start a command (login| stats)
functions : explain how we want to chart, compute and evaluate results (login| stats count)
arguments : variables we want to apply to the function
clause : how we want result grouped or defined

- All are not case sensitive. But command values are case sensitives

- Data interpretation
- Data classification
- Data enrichment
- Data normalization
- Data models

- classic dashboard: older and are best for quickly creating a simple, functional view of your data.
- Dashboard Studio: new, modern tool that gives you much more control and flexibility to create a professional and visually appealing dashboard.

- layout in dashboard studio : grid, absolute

- choose Grid for simplicity and automatic alignment, and choose Absolute for total creative control and custom design elements

Grafana vs splunk
------------------

- Grafana is primarily a visualization and monitoring tool. It’s excellent for creating real-time dashboards and connecting with multiple data sources like Prometheus, InfluxDB, Elasticsearch, etc.
- flexible for metrics and time-series monitoring
- real-time metrics monitoring and dashboards,

- splunk is not only visualizes data but also collects, indexes, searches, and analyzes machine data (logs, metrics, and traces) from almost any source.
- focus is log analysis, security, troubleshooting, or centralized data search, Splunk is better.

----------------------
video 2 - use field
----------------------
interested fields
# - numerals
a - string value

field search query
- field name : case sensitive
- value : not case sensitive
- field operators : =, != (with numerical/ string values)
		    >, >=, <, <= (with numerical values)

index=security sourcetype=linux_secure action=failure host!="mail*" 
index=security sourcetype=linux_secure action=failure NOT host="mail*" 

	index=security → Search only inside the security index (the storage area where security-related data/logs are kept).
	sourcetype=linux_secure → Look for events that came from the linux_secure sourcetype (usually Linux authentication or security logs).
	action=failure → Filter logs where the field action has the value failure (failed login attempts, failed actions, etc.).
	host!="mail"* → Exclude all events where the host field starts with mail (for example, mailserver1, mailhost2, etc.)

(index=web OR index=security) status!=200 : This only applies if the field status exists. If an event has no status field, it will not include.
(index=web OR index=security) NOT status=200 : This applies if the field not status exists also. If an event has no status field, it will include.

| field
----------
+ : include
- : exclude

index=web status IN ("500", "503", "505")
| fields status
| stats count by status

	Filter only events where the status field is 500, 503, or 505.
	keeps only the status field in the results and removes other fields.
	Count how many events occurred for each status

With fields status → Only status field is visible in the result.
Without it → Result is the same, but Splunk internally keeps extra fields.

|rename
--------
to rename fields in your search

index=web status IN ("500", "503", "505")
| fields status
| stats count by status
| rename status as "HTTP status" , count as "Number of occurrence"

| eval
--------
It’s like a calculator inside Splunk.

index=network sourcetype=cisco_wsa_squid
| stats sum(sc_bytes) as Bytes by usage
| eval bandwidth = Bytes/1024/1024

	Search inside the network index and Only look at logs that have the sourcetype cisco_wsa_squid
	Sum all values in sc_bytes field. Rename the result as Bytes and grouping by usage
	Convert the Bytes field into Megabytes (MB) by dividing by 1024/1024. come with as new field bandwidth

|erex 
--------------
Extract fields using examples

index=games sourcetype=SimCubeBeta
| erex Character fromfield=_raw examples="pixie, Kooby"
| where isnull(Character)

	Search in the games index and Filter logs with sourcetype SimCubeBeta
	extract a new field called Character from the raw event text (_raw).
	Splunk uses the given examples (pixie, Kooby) to figure out a regex pattern and extract similar values from events

|rex
---------
extract fields using a regex pattern

^[^'\n]*'(?P<User>[a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9.]+)'\s[a-zA-Z: ]+'(?P<Character>[a-zA-Z0-9.-]+)

explanation
^[^'\n]*'
	^ : start of the line
	[^'\n]* : any characters except ' or newline, repeated
	' : literal single quote
👉 This skips some initial text until it finds '.

(?P<User>[a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9.]+)
	(?P<User> … ) : named capture group called User
	[a-zA-Z0-9_.-]+ : username part (letters, numbers, dot, underscore, dash)
	@ : the @ symbol
	[a-zA-Z0-9-]+ : domain name (letters, numbers, dash)
	\. : dot
	[a-zA-Z0-9.]+ : domain extension (.com, .net, etc.)
👉 This captures an email address as the User field.

\s[a-zA-Z: ]+
	\s : a space
	[a-zA-Z: ]+ : some words (letters, colon, space)
👉 Example: “ plays character: ”

'(?P<Character>[a-zA-Z0-9.-]+)
	' : literal quote
	(?P<Character>[a-zA-Z0-9.-]+) : named capture group Character (letters, numbers, dot, dash)
👉 This captures the character name.

field alias
------------
A field alias is like giving a field an alternative name.
u can give same name to different fields.
u can searcg the field using alias name or its orginal name is possible

Field Extractions → Pull data out of raw logs.
Field Aliases → Give alternate names.
Calculated Fields → Derive new values.
Lookups → Enrich with external info.
Event Types → Categorize similar events.
Tags → Apply human-friendly labels.

--------------------------
video 3 - visualization
--------------------------
internal Splunk fields - _raw, _time

|table
--------
- transforming command
- make tabular
- field command improve efficiency of search over table command → Improves efficiency because it limits which fields

|dedup
-------
It only keeps the first occurrence of a duplicate and drops the rest.

|addtotals
------------
compute sum of all numeric field for each row

index=sales sourcetype=vendor_sales product_name=* VendorCountry="United States" OR VendorCountry="Canada"
| chart sum(price) over product_name by VendorCountry
| addtotals col=true label="Total Sales" labelfield="product_name" fieldname="Total By Product" row=false

|fieldformat
-------------
index=sales sourcetype=vendor_sales product_name=* VendorCountry="United States" OR VendorCountry="Canada"
| chart sum(price) over product_name by VendorCountry
| addtotals col=true label="Total Sales" labelfield="product_name" fieldname="Total By Product" row=false
|fieldformat "$" + tostring(Total, "commas")  -> output will be like $6,434

commands that used to transform search results to visualization
------------------------------------------------------------------
|top
-----
index=sales sourcetype=vendor_sales	
|top vendor sales=20  (without 20 it wil display top 10 results. limit=0 give all results)

|rare
------
- Finds the least common values of a field in your data (opposite of top).
- Useful when you want to see unusual or rare events

|stats
-------
- A general aggregation command to calculate metrics like count, sum, avg, max, etc.
- Does not require time; works on the dataset as is.

|chart
-------
- Similar to stats, but optimized for two-dimensional aggregation → X-axis and series.
- Output is structured for visual charts
- 2D aggregation for charting (x + series)

|timechart
----------
- Like chart, but time-aware.
- Automatically buckets data into time intervals (span=).
- Used for trending data over time.
- time-based aggregation (good for trends).

|trendline
------------
The trendline command is used in Splunk to add moving averages to your search results. It helps smooth out fluctuations and identify trends over time.
- SMA (Simple Moving Average)
     Formula: Average of the last n data points.
     Each point is weighted equally.
- EMA (Exponential Moving Average)
     Formula: Weighted average where recent data points have more weight.
     Reacts faster to new trends compared to SMA.
- WMA (Weighted Moving Average)
     Formula: Each point has a specific weight, usually giving more importance to recent data.
     Unlike EMA, the weights are linear (e.g., last 3 points: weights 1, 2, 3).

clauses
-------
as
by
over

|iplocation
------------
- The iplocation command looks up an IP address field and adds geographical information (like country, city, latitude, longitude, etc.) to your events.
- It uses a built-in GeoIP database in Splunk.
- adds new fields:
	Country
	City
	Region
	Latitude
	Longitude

|geostat
---------
- geostats is like stats, but geography-aware.
- It aggregates events by geographic location (latitude/longitude) and produces results that can be plotted on a map.
- Commonly used with iplocation.

choropleth map
------------------
- A choropleth map is a type of map where regions (countries, states, provinces, etc.) are shaded/colored based on data values.
- Example: Darker shade = more logins from that country.
- In Splunk, you use:
    iplocation → to convert IPs into geo info (Country, City, lat/long).
    geostats → to aggregate values by geo fields.
    geom → to overlay boundaries (like country outlines) and enable choropleth visualization.
- Cluster map → shows dots (geostats with lat/long).
- Choropleth map → colors regions (geom with geo lookups).

|geom
------
- The geom command in Splunk is used to draw geographic shapes (polygons) on a map, enabling choropleth visualizations.
- It matches your search results with a geo lookup dataset (like geo_countries or geo_us_states) and shades the corresponding region on the map.

single value visualizations
visual formatting
chart overlay - feature that lets you overlay multiple data series on the same chart for comparison.

-----------------------------
video 4 : working with time
-----------------------------
- Every event in Splunk has a field called _time.
- _time is the main timestamp field in Splunk.
- This represents when the event actually happened (taken from the log data).
- If Splunk can’t find a timestamp in the raw data, it uses the time when the event was indexed.

1. Searching by time
- You can choose a time range (last 15 minutes, last 7 days, custom, etc.)
- Example:
  index=web earliest=-24h latest=now

2.Formatting time
- You can display _time in different formats using strftime.
- Example:
  ... | eval date=strftime(_time, "%Y-%m-%d %H:%M:%S")

3. Extracting parts of time
- You can pull out just the hour, day, or month.
- Example:
  ... | eval hour=strftime(_time, "%H")

Timestamp in Raw Logs vs _time in Splunk
------------------------------------------
- Written inside the log/event itself - time stamp in raw logs
- Just a string inside the raw data, not directly usable for analysis 
- Splunk assigns _time as the index time (the moment the data was ingested) - splunk time

- @ : rounds down (snaps) to the nearest unit you specify.
Examples
--------
1. -30m@h
	Meaning: Go back 30 minutes, then snap to the nearest hour.
	If now is 09:45, Splunk snaps to 09:00.
	So it looks back to 09:00:00 on April 1, 2021.

2. earliest=-h@h
	Meaning: Set earliest time to 1 hour ago, snapped to the nearest hour.
	If now is 09:45, 1 hour ago is 08:45, snap to hour = 08:00.
	So it rounds down to 08:00:00 on April 1, 2021.

3. earliest=-mon@mon latest=@mon
	Meaning: From the start of the previous Monday to the start of this Monday.
	Example: If today is April 1 (Thursday), it will look back to 00:00:00 March 1, 2021 (Monday) and end at 00:00:00 April 1, 2021 (Monday).

4. earliest=-7d@d
	Meaning: 7 days ago, snapped to the beginning of that day.
	If today is April 1, 2021, it looks from 00:00:00 on March 25, 2021 to now (09:45:00 on April 1, 2021).

5. earliest=@d+3h
	Meaning: Start from today’s midnight (@d = 00:00 today), then add 3 hours.
	So on April 1, 2021 → it starts from 03:00:00 up to the current time (09:45:00)

Relative Time Modifiers ( <timeUnit> ) in Splunk
--------------------------------------------------
Current date & time - now
Second - s, sec, secs, second, seconds
Minute - m, min, minute, minutes
Hour - h, hr, hrs, hour, hours
Day - d, day, days
Week - w, week, weeks
Days of the week - w1 (Monday) … w6 (Saturday), w7 or w0 (Sunday) EG: earliest=@w1
Month - mon, month, months
Quarter - q, qtr, qtrs, quarter, quarters
Year - y, yr, yrs, year, years

date_*
-------
- When Splunk processes events, it automatically extracts the timestamp and then generates additional fields that start with date_*
- Events with timestamps have date_* fields
  Example: if your log says 2025-09-02 10:15:30, Splunk creates:
	date_hour=10
	date_mday=2
	date_month=9
	date_year=2025
	date_minute=15
	date_second=30
	date_wday=2 (Tuesday)

index=sales sourcetype=vendor_sales earliest=-2d@d latest=@d date_hour>=2 AND date_hour<5
| bin span=1h _time
| stats sum(price) as "Hourly Sales" by _time
| eval Hour = strftime(_time, "%b %d, %I %p")

explanation
------------
- index=sales sourcetype=vendor_sales → Look in the sales index for events with vendor_sales sourcetype.
- earliest=-2d@d → Start from 2 days ago at midnight.
- latest=@d → End at today’s midnight.
- date_hour>=2 AND date_hour<5 → Only take events between 2 AM and 4:59 AM.
- bin span=1h _time → Groups (bins) all events into 1-hour intervals based on _time.
- stats sum(price) as "Hourly Sales" by _time → Calculates the total sales amount (sum of price) per hour
- eval Hour = strftime(_time, "%b %d, %I %p")
	Creates a new field called Hour.
	Formats _time into a readable format:
	%b → Month short name (May)
	%d → Day of the month (12)
	%I → Hour (12-hour format)
	%p → AM/PM
Example output: May 12, 02 AM.

strftime(X, "format")
----------------------
Converts epoch time → human readable string.
Example:
	| eval hour = strftime(_time, "%b %d, %I %p")
	👉 Turns 1661941200 → Sep 01, 03 PM.

strptime("string", "format")
-----------------------------
Converts human readable string → epoch time.
Example:	
	| eval epoch_time = strptime("2021-04-20 16:00", "%Y-%m-%d %H:%M")
	👉 Turns 2021-04-20 16:00 → 1618934400
Format Variables in the Image
-----------------------------
⏰ Time
%H → Hour (00–23, 24-hour format)
%T → Time in HH:MM:SS format
%I → Hour (01–12, 12-hour format)
%M → Minute (00–59)
%p → AM or PM

📅 Days
%d → Day of month (01–31)
%w → Weekday number (0=Sunday → 6=Saturday)
%a → Abbreviated weekday (Sun)
%A → Full weekday (Sunday)
%F → Date in YYYY-MM-DD

📆 Months & Years
%b → Abbreviated month (Jan)
%B → Full month (January)
%m → Month number (01–12)
%Y → Year (2020)

- time bucket means grouping events into fixed time intervals.
  Example: instead of showing every raw event, you group them into 5-minute, 1-hour, or 1-day windows

- span is the keyword used to define the size of your time bucket.  
  Examples: span=10m → bucket events into 10-minute intervals.

timewrap - Splunk command that lets you compare data across different time periods on the same chart.
